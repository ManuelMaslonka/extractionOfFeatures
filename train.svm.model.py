# import pandas as pd
# import numpy as np
# from collections import Counter
# from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score
# from sklearn.preprocessing import StandardScaler
# from sklearn.svm import SVC
# from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
# from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
# from sklearn.decomposition import PCA
# from sklearn.pipeline import Pipeline
# from sklearn.ensemble import VotingClassifier, BaggingClassifier
# from sklearn.multiclass import OneVsRestClassifier
#
# # Na캜칤taj d치ta
# df = pd.read_csv('features_s_labelmi.csv')
#
# # Vstupn칠 d치ta (features) a cie쬺v칳 label
# X = df.drop(columns=['filename', 'label'])
# y = df['label']
#
# # Zak칩duj labely na 캜칤sla (napr. disco -> 0, blues -> 1, ...)
# from sklearn.preprocessing import LabelEncoder
# label_encoder = LabelEncoder()
# y_encoded = label_encoder.fit_transform(y)
#
# # Rozde na tr칠ningov칰 a testovaciu mno쬴nu
# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)
#
# # 맚andardiz치cia d치t (preto쬰 SVM je citliv칳 na 코k치lu vlastnost칤)
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)
#
# # Analyzuj distrib칰ciu tried
# class_counts = Counter(y_train)
# print("Distrib칰cia tried v tr칠novac칤ch d치tach:")
# for label, count in sorted(class_counts.items()):
#     print(f"  {label_encoder.classes_[label]}: {count}")
#
# # Vysk칰코aj r칪zne met칩dy feature selection
# print("\n游댌 Sk칰코am r칪zne met칩dy feature selection...")
# feature_selectors = {
#     'f_classif': SelectKBest(f_classif, k=50),
#     'mutual_info': SelectKBest(mutual_info_classif, k=50)
# }
#
# best_score = 0
# best_selector = None
# best_X_train = None
# best_X_test = None
#
# for name, selector in feature_selectors.items():
#     X_train_selected = selector.fit_transform(X_train_scaled, y_train)
#     X_test_selected = selector.transform(X_test_scaled)
#
#     # R칳chly test s jednoduch칳m SVM
#     svm = SVC(kernel='linear', C=1, class_weight='balanced')
#     scores = cross_val_score(svm, X_train_selected, y_train, cv=3, scoring='accuracy')
#     avg_score = np.mean(scores)
#     print(f"  {name}: Priemern칠 sk칩re = {avg_score:.4f}")
#
#     if avg_score > best_score:
#         best_score = avg_score
#         best_selector = selector
#         best_X_train = X_train_selected
#         best_X_test = X_test_selected
#
# print(f"游댌 Najlep코ia met칩da feature selection: {best_score:.4f}")
#
# # Aplikuj PCA pre redukciu dimenzionality
# print("\n游댌 Sk칰코am r칪zne konfigur치cie PCA...")
# pca_configs = [0.9, 0.95, 0.99]
# best_pca_score = 0
# best_pca = None
# best_X_train_pca = None
# best_X_test_pca = None
#
# for n_components in pca_configs:
#     pca = PCA(n_components=n_components)
#     X_train_pca = pca.fit_transform(best_X_train)
#     X_test_pca = pca.transform(best_X_test)
#
#     # R칳chly test s jednoduch칳m SVM
#     svm = SVC(kernel='linear', C=1, class_weight='balanced')
#     scores = cross_val_score(svm, X_train_pca, y_train, cv=3, scoring='accuracy')
#     avg_score = np.mean(scores)
#     print(f"  PCA(n_components={n_components}): Priemern칠 sk칩re = {avg_score:.4f}, Po캜et pr칤znakov: {X_train_pca.shape[1]}")
#
#     if avg_score > best_pca_score:
#         best_pca_score = avg_score
#         best_pca = pca
#         best_X_train_pca = X_train_pca
#         best_X_test_pca = X_test_pca
#
# print(f"游댌 Najlep코ia konfigur치cia PCA: {best_pca_score:.4f} s {best_X_train_pca.shape[1]} pr칤znakmi")
#
# # Pou쬴j najlep코ie transformovan칠 d치ta
# X_train_pca = best_X_train_pca
# X_test_pca = best_X_test_pca
#
# print(f"P칪vodn칳 po캜et pr칤znakov: {X_train_scaled.shape[1]}")
# print(f"Po캜et pr칤znakov po SelectKBest: {X_train_selected.shape[1]}")
# print(f"Po캜et pr칤znakov po PCA: {X_train_pca.shape[1]}")
#
# # Roz코칤ren칳 grid search pre SVM - optimalizovan칳 pre hudobn칰 klasifik치ciu
# print("\n游댌 Optimalizujem hyperparametre SVM...")
#
# # Najprv zist칤me, ktor칳 kernel je najs쬿bnej코칤
# kernel_test = {
#     'kernel': ['linear', 'rbf', 'poly'],
#     'C': [1],
#     'class_weight': ['balanced']
# }
#
# kernel_search = GridSearchCV(
#     SVC(probability=True, random_state=42),
#     kernel_test,
#     cv=3,
#     scoring='accuracy',
#     n_jobs=-1
# )
# kernel_search.fit(X_train_pca, y_train)
# best_kernel = kernel_search.best_params_['kernel']
# print(f"游댌 Najs쬿bnej코칤 kernel: {best_kernel}")
#
# # Teraz detailnej코ie preh쬬d치me hyperparametre pre najlep코칤 kernel
# if best_kernel == 'linear':
#     param_grid_svm = {
#         'kernel': ['linear'],
#         'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100],
#         'class_weight': ['balanced', None],
#         'decision_function_shape': ['ovo', 'ovr']
#     }
# elif best_kernel == 'rbf':
#     param_grid_svm = {
#         'kernel': ['rbf'],
#         'C': [0.1, 0.5, 1, 5, 10, 50, 100],
#         'gamma': ['scale', 'auto', 0.0001, 0.001, 0.01, 0.1, 1],
#         'class_weight': ['balanced', None],
#         'decision_function_shape': ['ovo', 'ovr']
#     }
# elif best_kernel == 'poly':
#     param_grid_svm = {
#         'kernel': ['poly'],
#         'C': [0.1, 1, 10, 100],
#         'gamma': ['scale', 'auto', 0.01, 0.1, 1],
#         'degree': [2, 3, 4, 5],
#         'coef0': [0, 0.1, 1],
#         'class_weight': ['balanced', None],
#         'decision_function_shape': ['ovo', 'ovr']
#     }
# else:
#     param_grid_svm = {
#         'kernel': ['sigmoid'],
#         'C': [0.1, 1, 10, 100],
#         'gamma': ['scale', 'auto', 0.01, 0.1, 1],
#         'class_weight': ['balanced', None],
#         'decision_function_shape': ['ovo', 'ovr']
#     }
#
# # Pou쬴jeme GridSearchCV na optimaliz치ciu hyperparametrov s kr칤쬺vou valid치ciou
# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# grid_search_svm = GridSearchCV(
#     SVC(probability=True, random_state=42),
#     param_grid_svm,
#     cv=cv,
#     scoring='accuracy',
#     n_jobs=-1,  # pou쬴j v코etky dostupn칠 jadr치
#     verbose=1
# )
# grid_search_svm.fit(X_train_pca, y_train)
#
# # Najlep코ie parametre
# print("游꿢 Najlep코ie parametre pre SVM:", grid_search_svm.best_params_)
# print("游꿢 Najlep코ie sk칩re po캜as grid search:", grid_search_svm.best_score_)
#
# # Vytvor ensemble model s najlep코칤m SVM
# best_svm = grid_search_svm.best_estimator_
#
# # Vytvor bagging ensemble s najlep코칤m SVM
# # V nov코칤ch verzi치ch scikit-learn sa parameter 'base_estimator' zmenil na 'estimator'
# bagging_svm = BaggingClassifier(
#     estimator=best_svm,
#     n_estimators=10,
#     random_state=42,
#     n_jobs=-1
# )
# bagging_svm.fit(X_train_pca, y_train)
#
# # Predikcia a hodnotenie pre z치kladn칳 SVM
# y_pred_svm = best_svm.predict(X_test_pca)
# print("游꿢 SVM - Presnos콘:", accuracy_score(y_test, y_pred_svm))
# print("\n游늵 SVM - Report:\n", classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))
#
# # Predikcia a hodnotenie pre bagging SVM
# y_pred_bagging = bagging_svm.predict(X_test_pca)
# print("游꿢 Bagging SVM - Presnos콘:", accuracy_score(y_test, y_pred_bagging))
# print("\n游늵 Bagging SVM - Report:\n", classification_report(y_test, y_pred_bagging, target_names=label_encoder.classes_))
#
# # Vytvor a zobraz confusion matrix
# cm = confusion_matrix(y_test, y_pred_bagging)
# print("\n游늵 Confusion Matrix:")
# for i, row in enumerate(cm):
#     print(f"{label_encoder.classes_[i]}: {row}")


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA

# Load data
df = pd.read_csv('features_s_labelmi.csv')

# Separate features and target
X = df.drop(columns=['filename', 'label'])
y = df['label']

# Encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Standardize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply the best feature selection method (f_classif with k=50)
selector = SelectKBest(f_classif, k=50)
X_train_selected = selector.fit_transform(X_train_scaled, y_train)
X_test_selected = selector.transform(X_test_scaled)

# Apply PCA with n_components=0.99 (resulted in 37 features)
pca = PCA(n_components=0.99)
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

# Create the SVM model with the best parameters
best_svm = SVC(
    kernel='rbf',
    C=5,
    gamma='auto',
    class_weight=None,
    decision_function_shape='ovo',
    probability=True,
    random_state=42
)

# Train the model
best_svm.fit(X_train_pca, y_train)

# Evaluate the model
y_pred = best_svm.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)
print(f"游꿢 SVM Accuracy: {accuracy:.4f}")
print("\n游늵 Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))